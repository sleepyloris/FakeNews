{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e32672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c06e787",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n",
    "wv = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ac0eb0",
   "metadata": {},
   "source": [
    "**Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a52603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text summarization\n",
    "def summarize(doc):\n",
    "    keyword = []\n",
    "    stopwords = list(STOP_WORDS)\n",
    "    pos_tag = ('PROPN', 'ADJ', 'NOUN', 'VERB')\n",
    "    for token in doc:\n",
    "        if (token.text in stopwords or token.text in punctuation):\n",
    "            continue\n",
    "        if (token.pos_ in pos_tag):\n",
    "            keyword.append(token.text)\n",
    "    \n",
    "    freq_word = Counter(keyword)\n",
    "    max_freq = Counter(keyword).most_common(1)[0][1]\n",
    "    for word in freq_word.keys():\n",
    "        freq_word[word] = (freq_word[word]/max_freq)\n",
    "    freq_word.most_common(5)\n",
    "    \n",
    "\n",
    "    sent_strength = {}\n",
    "    for sent in doc.sents:\n",
    "        for word in sent:\n",
    "            if word.text in freq_word.keys():\n",
    "                if sent in sent_strength.keys():\n",
    "                    sent_strength[sent]+=freq_word[word.text]\n",
    "                else:\n",
    "                    sent_strength[sent]=freq_word[word.text]\n",
    "    summarized_sents = nlargest(3, sent_strength, key=sent_strength.get)\n",
    "    \n",
    "    return summarized_sents[0].as_doc()\n",
    "\n",
    "#text lemmatization\n",
    "def lemmatize(text):\n",
    "    return ' '.join([x.lemma_ for x in text])\n",
    "\n",
    "#stopword removal\n",
    "def stop_removal(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    doc_summary = nlp(text)\n",
    "    tmp_list = []\n",
    "    for token in doc_summary:\n",
    "        if not token.is_stop:\n",
    "            tmp_list.append(token)\n",
    "    without_stops = ''.join([token.text_with_ws for token in tmp_list])\n",
    "    return without_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4aceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(db_location=\"data\\Fake.csv\",limit=0,exportName=\"fake.txt\"):\n",
    "    \n",
    "    df = pd.read_csv(db_location)\n",
    "\n",
    "    #Stop:\n",
    "    print(\"converting \"+str(limit)+\" out of \"+str(len(df[\"text\"])))\n",
    "\n",
    "    i = 0\n",
    "    cumulativeArticles = str(\"\")\n",
    "    for n in fdf[\"text\"]:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        n = nlp(n)\n",
    "    \n",
    "        n = summarize(n)\n",
    "        n = lemmatize(n)\n",
    "        n = stop_removal(n)\n",
    "\n",
    "        cumulativeArticles += n\n",
    "        print(str(i)+\"/\"+str(len(fdf[\"text\"]))) \n",
    "        i +=1\n",
    "        if (i > limit) and (limit !=0):\n",
    "            break\n",
    "\n",
    "    text_file = open(exportName, \"w\")\n",
    "    text_file.write(cumulativeArticles)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4566a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(\"data\\Fake.csv\",0,\"fake.txt\")\n",
    "preprocess(\"data\\True.csv\",0,\"true.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff864501",
   "metadata": {},
   "source": [
    "**Data Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e6d533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject_phrase(doc):\n",
    "    for token in doc:\n",
    "        if (\"subj\" in token.dep_):\n",
    "            subtree = list(token.subtree)\n",
    "            start = subtree[0].i\n",
    "            end = subtree[-1].i + 1\n",
    "            return doc[start:end]\n",
    "        \n",
    "def get_object_phrase(doc):\n",
    "    for token in doc:\n",
    "        if (\"dobj\" in token.dep_):\n",
    "            subtree = list(token.subtree)\n",
    "            start = subtree[0].i\n",
    "            end = subtree[-1].i + 1\n",
    "            return doc[start:end]\n",
    "\n",
    "def get_verb_phrase(doc):\n",
    "    for token in doc:\n",
    "        if (\"ROOT\" in token.dep_):\n",
    "            return token     \n",
    "    \n",
    "def get_whole_phrase(doc,index):\n",
    "    if(doc[index].dep_ == \"ROOT\"):\n",
    "        subtree = list(doc[index].subtree)\n",
    "        start = subtree[0].i\n",
    "        end = subtree[-1].i + 1\n",
    "        return doc[start:end]\n",
    "\n",
    "def docToSentences(doc): #list of sentences\n",
    "    sentences = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if (\"ROOT\" in token.dep_):\n",
    "            whole_phrase = get_whole_phrase(doc,token.i)\n",
    "            subj_phrase = get_subject_phrase(whole_phrase)\n",
    "            obj_phrase = get_object_phrase(whole_phrase)\n",
    "            sentences.append([subj_phrase,token,obj_phrase])\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def sentenceToParts(sentence): # list of parts of one sentence\n",
    "    parts = []\n",
    "    subj_phrase = get_subject_phrase(sentence)\n",
    "    obj_phrase = get_object_phrase(sentence)\n",
    "    verb = get_verb_phrase(sentence)\n",
    "    \n",
    "    return [subj_phrase,verb,obj_phrase]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ce7d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_distance(word1,word2): #strings\n",
    "    \n",
    "    if((word1 not in wv.key_to_index) or (word2 not in wv.key_to_index) ):\n",
    "        return 1\n",
    "    return wv.distance(word1.text,word2.text)      \n",
    "\n",
    "def partDistance(a,b):\n",
    "    totalmin = 0    \n",
    "\n",
    "    if(a == None or b == None):\n",
    "        return 1 #nice number chosen by team\n",
    "    \n",
    "    if(type(a) == spacy.tokens.token.Token):\n",
    "        a = [a]\n",
    "\n",
    "    if(type(b) == spacy.tokens.token.Token):\n",
    "        b = [b]\n",
    "\n",
    "    for item_a in a:\n",
    "        for item_b in b:\n",
    "            localmin = 1\n",
    "            if split_distance(item_a,item_b) < localmin:\n",
    "                localmin = split_distance(item_a,item_b)\n",
    "        totalmin += localmin\n",
    "    return totalmin\n",
    "\n",
    "def sentenceDistance(doc,doc2):\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(doc)\n",
    "    doc2 = nlp(doc2)\n",
    "\n",
    "    s1 = sentenceToParts(doc)\n",
    "    s2 = sentenceToParts(doc2)\n",
    "    \n",
    "    totalDistance = 0\n",
    "    for i in range(3):\n",
    "        totalDistance += partDistance(s1[i],s2[i])\n",
    "    return totalDistance/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f2e318",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Procesisng pipeline\n",
    "from spacy.lang.en import English\n",
    "\n",
    "def sentenceRatio(sentence,db): # two strings\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp = English()\n",
    "    nlp.add_pipe(\"sentencizer\")    \n",
    "    \n",
    "    db = nlp(db)\n",
    "    \n",
    "    total = 0\n",
    "    i = 0\n",
    "    for sentenceFromdb in list(db.sents):\n",
    "        sentenceFromdb = [s for s in str(sentenceFromdb) if s.isalnum() or s.isspace()]\n",
    "        sentenceFromdb = \"\".join(sentenceFromdb)    \n",
    "        \n",
    "        total += sentenceDistance(sentence,sentenceFromdb)\n",
    "        print(str(total)+\" \"+str(i)+\"/\"+str(len(list(db.sents))))\n",
    "        i +=1\n",
    "            \n",
    "\n",
    "    return (total/len(list(db.sents)))\n",
    "\n",
    "\n",
    "def articleRatio(article,db): #two strings\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp = English()\n",
    "    nlp.add_pipe(\"sentencizer\")    \n",
    "    \n",
    "    article = nlp(article)    \n",
    "    \n",
    "    total = 0\n",
    "    for sentence in list(article.sents):\n",
    "        total += sentenceRatio(str(sentence),str(db))\n",
    "        \n",
    "    rvalue =  total / (len(list(article.sents)))\n",
    "    return rvalue\n",
    "\n",
    "                       \n",
    "def start(fakedb,article):                       \n",
    "    f = open(fakedb, \"r\")\n",
    "    textfile = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    a = open(article, \"r\")\n",
    "    articlefile = a.read()\n",
    "    a.close()    \n",
    "\n",
    "    ratio = articleRatio(article,fakedb)\n",
    "    \n",
    "    print(ratio)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62661b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start(\"fake.txt\",\"testArticle.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
